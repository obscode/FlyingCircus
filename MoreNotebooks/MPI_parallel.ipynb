{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "personalized-husband",
   "metadata": {},
   "source": [
    "# MPI for python \n",
    "\n",
    "In this notebook, you will learn how to run your python code on many cores using Message Pasing Interface (MPI).  This is the type of \"parallel computing\" you should use to scale your code for running on \"high performance computing\" systems like Mies or Memex at carnegie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-vanilla",
   "metadata": {},
   "source": [
    "## What is parallel programming ?\n",
    "\n",
    "If you run your ordinary python (or any other language) without any parallelization, in practice, it will use only 1 core of your machine while you have many more available. The purpose of parallelzation of a code is to use as many cores as possible to spped up your code. If you parallelize it in an efficient way, the speed-up will be roughly proportional to # of cores used. It is a great benifit when you have access to super-computers which have hunderds of cores avialable for you. \n",
    "\n",
    "\n",
    "Parallelize your code and get more work done !\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-hanging",
   "metadata": {},
   "source": [
    "## Quick review of a computer's structure :\n",
    "\n",
    "A stand-alone computer like your current desktop is called a $\\textbf{node}$ which contains few $\\textbf{cores}$ (also clalled CPUs or $\\textbf{processors}$). By default, the memory is shared among all these cores for the purpose of holding data while running your code. e.g. All cores know about the value of a variable which was calculated on any of the cores within the same node.\n",
    "\n",
    "This picture illustrates 2 independant nodes, or computers, being connected via a network. In this example, each node contains 3 cores which share memory among themselves. On the other hand, the two separate nodes do not have shared memory. For example, if a core on the right node needs to know the value of a variable calculated on a core locacted on the other node, you need to send that data within the network connecting these two nodes. Therefore, communication between cores on different nodes are slower than cores within the same node.\n",
    "\n",
    "![](./node-core.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-yellow",
   "metadata": {},
   "source": [
    "## What are the options ?\n",
    "\n",
    "\n",
    "There are 3 options to make your code parallel :\n",
    "\n",
    "1. $\\textbf{Simplest and fastests way}$ : If you want to run the same code on different data chunks, you can simply run your code multiple times and each time feed one chunck of the data to it. Each time you run your code, a single core is hookep up to run your code on it.\n",
    "\n",
    "\n",
    "2. $\\textbf{Shared-Memory parallelization}$: In this approach you let your machine distribute iterative commands like loops among the available cores. If you have 1000 iterations in a loop and 4 cores available, it runs 250 of them on each and at the end it will add up the results from each core to return a single final result. It is called $\\textbf{shared memory}$ parallelization since the memory is physically shared among cores and you do not need to put any effort to communicate between them, i.e. they already know how to combine the results from each core to give you the final result. If you ever hear about $\\textbf{threading}$ or $\\textbf{Open-MP}$, it means this type of parallelization. The down side of this method is that you can never use more than a certain muber of cores (number of cores on a single node). Therefore, not the best option for running your code on clusters like Mies.\n",
    "\n",
    "$\\textbf{Bad News :}$ The structure of python language is not compatible with shared memory approach, therefore there is not any real implementation of this method for python. There are some work-arounds like [multi processing](https://docs.python.org/3/library/multiprocessing.html). Altough they are so easy to implement, it is not very natural to python and therefore so very limiting unlike the next method.\n",
    "\n",
    "\n",
    "3. $\\textbf{MPI}$ : It stands for $\\textbf{Message Passing Interface}$ : In this case, you can run your code on as many as cores you want even if they are on different nodes. This is the only option when you want to use more than one node. By choosing number of processes (usually one process per core) lets call it np, machine runs a copy of your code on each processes and labels them by an integer called $\\textbf{rank}$ (so number of ranks is the same as number of processes). Also, it establishes a $\\textbf{communicator}$ to transfer in-memory data between different ranks. In MPI, none of the ranks share memory and therefore if you need them to communicate, you have to specify it in your code. \n",
    "\n",
    "Comparing MPI with shared memory or threaded method :\n",
    "\n",
    "![0.2](./MPI-threaded-serial.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-format",
   "metadata": {},
   "source": [
    "## Let's use MPI :\n",
    "\n",
    "1.  First, you need to make sure you have MPI installed. List all the modules available on your system :\n",
    "\n",
    "Mac:\n",
    "\n",
    "```brew list```\n",
    "\n",
    "Linux:\n",
    "\n",
    "```module list```\n",
    "\n",
    "If you find $\\textbf{MPICH}$ on that list, you're good. If not, you should install it :\n",
    "\n",
    "Mac:\n",
    "\n",
    "``` brew install mpich ```\n",
    "\n",
    "Linux (fedora) :\n",
    "\n",
    "``` dnf install  mpich ```\n",
    "\n",
    "Linux (Ubuntu/Debian) :\n",
    "\n",
    "``` sudo apt install mpich ```\n",
    "\n",
    "2. Fortunately a stable MPI package is avaiable for python, [mpi4py](https://mpi4py.readthedocs.io/en/stable/). Install this via conda :\n",
    "\n",
    "``` conda install -c anaconda mpi4py ```\n",
    "\n",
    "Now we are ready to go !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-moral",
   "metadata": {},
   "source": [
    "### Hello World example :\n",
    "\n",
    "The simplest example is asking each rank to print their rank id. Type/copy the code below in a file named `hello_world.py` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am rank  0  among  1  available ranks!\n",
      "Rank  0  : You can command me privately by specifying my id !\n"
     ]
    }
   ],
   "source": [
    "import mpi4py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# The communicator among ranks\n",
    "comm = MPI.COMM_WORLD\n",
    "# Total number of ranks\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print('Hi, I am rank ', rank, ' among ', size, ' available ranks!')\n",
    "\n",
    "if rank == 0:\n",
    "    print('Rank ', rank, ' : You can command me privately by specifying my id !')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-opportunity",
   "metadata": {},
   "source": [
    "\n",
    "Then, you need to run your script on terminal like :\n",
    "\n",
    "` mpirun -np 2 python hellow_world.py`\n",
    "\n",
    "where `-np 2` specifies number of processors you want to use. To see how many cores you have on any Unix machine you can use `htop` package. Type `htop` on terminal and see if you have it installed. If not, you can install it with :\n",
    "\n",
    "mac :\n",
    "\n",
    "` brew install htop`\n",
    "\n",
    "Linux (fedora) :\n",
    "\n",
    "`dnf install htop`\n",
    "\n",
    "Linux (Ubunto/Debian)\n",
    "\n",
    "` sudo apt install htop`\n",
    "\n",
    "Here is a snapshot of what you're expected to see. It shows my eesktop has 12 cores and none of them are busy.\n",
    "\n",
    "![htop](htop.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-germany",
   "metadata": {},
   "source": [
    "## Comunicating among ranks :\n",
    "\n",
    "\n",
    "MPI has so many buit-in functions to help you communicate among ranks. There are 2 general type of communications :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-litigation",
   "metadata": {},
   "source": [
    "### Ponit-to-point communications :\n",
    "\n",
    "Sometimes you need to send a scalar, numpy arrary or a python object from one rank to another rank. It can be done with functions below :\n",
    "\n",
    "- `send() and recieve()` : For transferring scalars and python objects\n",
    "\n",
    "- `Send() and Recieve()` : For transfering numpy arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-script",
   "metadata": {},
   "source": [
    "#### Example : \n",
    "\n",
    "(Credit : [mpi4py documentaion](https://mpi4py.readthedocs.io/en/stable/tutorial.html#point-to-point-communication) )\n",
    "\n",
    "Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpi4py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# The communicator among ranks\n",
    "comm = MPI.COMM_WORLD\n",
    "# Total number of ranks\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1, tag=11)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=11)\n",
    "    print('rank ', rank, 'data = ', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-cannon",
   "metadata": {},
   "source": [
    "NumPy arrays (the fast way!) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpi4py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "# The communicator among ranks\n",
    "comm = MPI.COMM_WORLD\n",
    "# Total number of ranks\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# passing MPI datatypes explicitly\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype='i')\n",
    "    comm.Send([data, MPI.INT], dest=1, tag=77)\n",
    "elif rank == 1:\n",
    "    # You need a buffer for the array\n",
    "    data = np.empty(10, dtype='i')\n",
    "    comm.Recv([data, MPI.INT], source=0, tag=77)\n",
    "    print('Rank ', rank, ' data ', data)\n",
    "\n",
    "# automatic MPI datatype discovery\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype=np.float64)\n",
    "    comm.Send(data, dest=1, tag=13)\n",
    "elif rank == 1:\n",
    "    data = np.empty(10, dtype=np.float64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    print('Rank ', rank, ' data. ', data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-outreach",
   "metadata": {},
   "source": [
    " - Tip : All functions for communicating Numpy arrays srtart with capital letters. like `Send()` and `Recieve()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-improvement",
   "metadata": {},
   "source": [
    "### Collective communications :\n",
    "\n",
    "On this type of communication, one rank can sned/recieve to/from all ranks. So, all ranks are involved. \n",
    "\n",
    "Examples are :\n",
    "\n",
    "` bcast(), Bcast()` : Send a python object or numpy array from one rank to all other ranks :\n",
    "\n",
    "` scatter(), Scatter()` : Scatter a list or numpy array among all ranks \n",
    "\n",
    "`  gather(), Gather() ` : Gather obejcts or numpy arrays in a single list or numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-child",
   "metadata": {},
   "source": [
    "#### Examples :\n",
    "\n",
    "(Credit : [mpi4py documentaion](https://mpi4py.readthedocs.io/en/stable/tutorial.html#point-to-point-communication) )\n",
    "\n",
    "Broadcasting for python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spoken-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank : 0 data = {'key1': [7, 2.72, (2+3j)], 'key2': ('abc', 'xyz')}\n"
     ]
    }
   ],
   "source": [
    "import mpi4py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# The communicator among ranks\n",
    "comm = MPI.COMM_WORLD\n",
    "# Total number of ranks\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'key1' : [7, 2.72, 2+3j],\n",
    "            'key2' : ( 'abc', 'xyz')}\n",
    "else:\n",
    "    # You need a buffer\n",
    "    data = None\n",
    "data = comm.bcast(data, root=0)\n",
    "print('Rank :', rank, 'data =', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-addiction",
   "metadata": {},
   "source": [
    "Scattering for python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spare-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = [(i+1)**2 for i in range(size)]\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.scatter(data, root=0)\n",
    "print('Rank ', rank, 'data = ', data)\n",
    "assert data == (rank+1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-zambia",
   "metadata": {},
   "source": [
    "Gathering python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "roman-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 data = [1]\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "data = (rank+1)**2\n",
    "data = comm.gather(data, root=0)\n",
    "\n",
    "    \n",
    "print('Rank', rank, 'data =', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-spine",
   "metadata": {},
   "source": [
    "Numpy arrays :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "structured-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype='i')\n",
    "    # You need to make sure the array's elements are contiguius in memory\n",
    "    data = np.ascontiguousarray(data)\n",
    "\n",
    "else:\n",
    "    # Make a buffer for array\n",
    "    data = np.empty(10, dtype='i')\n",
    "    # You need to make sure the array's elements are contiguius in memory\n",
    "    data = np.ascontiguousarray(data)\n",
    "\n",
    "comm.Bcast(data, root=0)\n",
    "for i in range(10):\n",
    "    assert data[i] == i\n",
    "print('Rank ', rank, ' data ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "sendbuf = None\n",
    "if rank == 0:\n",
    "    sendbuf = np.empty([size, 10], dtype='i')\n",
    "    sendbuf.T[:,:] = range(size)\n",
    "    # You need to make sure the array's elements are contiguius in memory\n",
    "    sendbuf = np.ascontiguousarray(sendbuf)\n",
    "\n",
    "recvbuf = np.empty(10, dtype='i')\n",
    "# You need to make sure the array's elements are contiguius in memory\n",
    "recvbuf = np.ascontiguousarray(recvbuf)\n",
    "\n",
    "print('Rank ', rank, ' sendbuf ', sendbuf)\n",
    "comm.Scatter(sendbuf, recvbuf, root=0)\n",
    "print('Rank ', rank, ' recvbuf ', recvbuf)\n",
    "assert np.allclose(recvbuf, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "sendbuf = np.zeros(10, dtype='i') + rank\n",
    "# You need to make sure the array's elements are contiguius in memory\n",
    "sendbuf = np.ascontiguousarray(sendbuf)\n",
    "print('Rank ', rank, ' sendbuf ' , sendbuf)\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty([size, 10], dtype='i')\n",
    "    # You need to make sure the array's elements are contiguius in memory\n",
    "    recvbuf = np.ascontiguousarray(recvbuf)\n",
    "comm.Gather(sendbuf, recvbuf, root=0)\n",
    "\n",
    "print('Rank ', rank, ' recvbuf ' , recvbuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "\n",
    "data = np.arange(10)*0.1*rank\n",
    "# You need to make sure the array's elements are contiguius in memory\n",
    "data = np.ascontiguousarray(data)\n",
    "print('Rank ', rank, ' data before allreduce ', data)\n",
    "comm.Allreduce(MPI.IN_PLACE, data, op=MPI.SUM)\n",
    "print('Rank ', rank, ' data after  allreduce ', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-baltimore",
   "metadata": {},
   "source": [
    "- There are many other built-in functions that are optimized for minimal computaion and memory usage. [This website](https://pages.tacc.utexas.edu/~eijkhout/pcse/html/index.html) is a great resource for learning more about MPI and all other useful communications. These functions behave similarly in python, C and Fortran, but the syntax is slightly different in each. This website has examples for all 3  languages on each topic. \n",
    "\n",
    "- I also recommend you to look at [mpi4py documentaion](https://mpi4py.readthedocs.io/en/stable/tutorial.html#point-to-point-communication) and any other resource you might found on google."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
